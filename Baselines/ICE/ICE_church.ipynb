{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3819be82",
   "metadata": {},
   "source": [
    "We use the official implementation of ICE available here: https://github.com/zhangrh93/InvertibleCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fbc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f499e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt  \n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchsummary import summary\n",
    "import shutil\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fb4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68effe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model = model.eval().to(device)\n",
    "g = nn.Sequential(*(list(model.children())[:-2]))  # All layers except the last two\n",
    "h = lambda x: model.fc(torch.mean(x, (2, 3))) \n",
    "h_2d = lambda x: model.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399196fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_list = os.listdir('dataset/ChurchFolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64f419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.imgs[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = img.permute(1,2,0)\n",
    "        return img,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c25d6",
   "metadata": {},
   "source": [
    "### Parameters and Methods initialization\n",
    "\n",
    "Stanford Dogs Dataset is a subset of the ImageNet. The only thing we need to do is to connect the classes from Stanford Dogs Dataset to ImageNet by class ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b539ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageNet_ID2idx = {}\n",
    "ImageNet_idx2ID = []\n",
    "with open('synset_words.txt','r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        tsplit = line.split('\\n')[0].split(' ')\n",
    "        ID = tsplit[0]\n",
    "        name = ' '.join(tsplit[1:])\n",
    "        #print(ID)\n",
    "\n",
    "        ImageNet_ID2idx[ID] = i\n",
    "        ImageNet_idx2ID.append(ID)\n",
    "\n",
    "fpath = Path('dataset') / 'ChurchFolder'\n",
    "dog_paths = {d:fpath / d for d in dog_list}\n",
    "\n",
    "dog_dict = {}\n",
    "fpath = Path('dataset') / 'ChurchFolder'\n",
    "dog_name2idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e48128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,k in enumerate(dog_paths.keys()):\n",
    "  ID = k.split('-')[0]\n",
    "  name = k.split('-')[1]\n",
    "  if ID in ImageNet_idx2ID:\n",
    "    dog_dict[ImageNet_ID2idx[ID]] = {'idx':ImageNet_ID2idx[ID],'ID':ID,'name':name,'path':fpath / k}\n",
    "    dog_name2idx[k] = ImageNet_ID2idx[ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e42684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{497: {'idx': 497,\n",
       "  'ID': 'n03028079',\n",
       "  'name': 'church',\n",
       "  'path': PosixPath('dataset/ChurchFolder/n03028079-church')}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f32e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = {}\n",
    "target_class = 497\n",
    "paras['target_classes'] = [497]\n",
    "paras['classes_names'] = ['church']\n",
    "paras['n_components'] = 25\n",
    "paras['layer_name'] = 'layer4'\n",
    "paras['title'] = \"church\"\n",
    "paras['overwrite'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e6ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imgs(paths = [],labels = []):\n",
    "    imgs = []\n",
    "    for i,path in enumerate(paths):\n",
    "        imgs += [(os.path.join(path,t),labels[i]) for t in os.listdir(path)]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2328d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(target_classes = []):\n",
    "  loaders = []\n",
    "  for i,idx in enumerate(target_classes):\n",
    "    loaders.append(DataLoader(\n",
    "            ImageDataset(make_imgs([dog_dict[idx]['path']],[i]),\n",
    "                    transforms.Compose([\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225]),\n",
    "                    ])),\n",
    "            batch_size = 16,\n",
    "            num_workers=2,\n",
    "            #shuffle=True\n",
    "    ))\n",
    "  return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25ecd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self,model,batch_size = 128):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_feature(self,x,layer_name):\n",
    "        '''\n",
    "        get feature map from a given layer\n",
    "        '''\n",
    "        pass\n",
    "    def feature_predict(self,feature,layer_name = None):\n",
    "        '''\n",
    "        prediction from given feature maps\n",
    "        '''\n",
    "        pass  \n",
    "    def predict(self,x):\n",
    "        '''\n",
    "        provide prediction from given feature\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "\n",
    "class PytorchModelWrapper(ModelWrapper):   \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 layer_dict = {},\n",
    "                 predict_target = None,\n",
    "                 input_channel_first = False, # True if input image is channel first\n",
    "                 model_channel_first = True, #True if model use channel first\n",
    "                 #switch_channel = None, #\"f_to_l\" or \"l_to_f\" if switch channel is required from loader to model\n",
    "                 numpy_out = True,\n",
    "                 input_size = [3,224,224], #model's input size\n",
    "                 batch_size=128):#target: (layer_name,unit_nums)\n",
    "\n",
    "        super().__init__(model,batch_size)\n",
    "        self.layer_dict = layer_dict\n",
    "        self.layer_dict.update(dict(model.named_children()))\n",
    "        self.predict_target = predict_target\n",
    "        self.input_channel = 'f' if input_channel_first else 'l'\n",
    "        self.model_channel = 'f' if model_channel_first else 'l'\n",
    "        self.numpy_out = numpy_out\n",
    "        self.input_size = list(input_size)\n",
    "        \n",
    "        self.non_negative = False\n",
    "\n",
    "        self.CUDA = torch.cuda.is_available()\n",
    "\n",
    "    def _to_tensor(self,x):\n",
    "        if type(x) == np.ndarray:\n",
    "            x = torch.from_numpy(x)\n",
    "        x = torch.clone(x)\n",
    "        if x.ndim == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def _switch_channel_f_to_l(self,x): #transform from channel first to channel last\n",
    "        if x.ndim == 3:\n",
    "            x = x.permute(1,2,0)\n",
    "        if x.ndim == 4:\n",
    "            x = x.permute(0,2,3,1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _switch_channel_l_to_f(self,x): #transform from channel last to channel first\n",
    "        if x.ndim == 3:\n",
    "            x = x.permute(2,0,1)\n",
    "        if x.ndim == 4:\n",
    "            x = x.permute(0,3,1,2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _switch_channel(self,x,layer_in='input',layer_out='output',to_model=True):\n",
    "        c_from = None\n",
    "        c_to = None\n",
    "        if to_model:\n",
    "            c_from = self.input_channel if layer_in == 'input' else 'l'            \n",
    "            c_to = self.model_channel\n",
    "        else:\n",
    "            c_from = self.model_channel\n",
    "            c_to = 'l'\n",
    "\n",
    "        #print (x.shape,c_from,c_to,layer_in,layer_out,to_model)\n",
    "\n",
    "        if c_from == 'f' and c_to == 'l':\n",
    "            x = self._switch_channel_f_to_l(x)\n",
    "        if c_from == 'l' and c_to == 'f':\n",
    "            x = self._switch_channel_l_to_f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _fun(self,x,layer_in = \"input\",layer_out = \"output\"):\n",
    "        #tensor cpu in cpu out\n",
    "\n",
    "        x = x.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "        in_flag = False\n",
    "        if layer_in == \"input\":\n",
    "            in_flag = True\n",
    "        \n",
    "\n",
    "        data_in = x.clone()\n",
    "        if self.CUDA:\n",
    "            data_in = data_in.cuda()\n",
    "        data_out = []\n",
    "\n",
    "        handles = []\n",
    "        \n",
    "        def hook_in(m,i,o):\n",
    "            return data_in\n",
    "        def hook_out(m,i,o):\n",
    "            data_out.append(o)\n",
    "\n",
    "        if layer_in == \"input\":\n",
    "            nx = x\n",
    "        else:\n",
    "            handles.append(self.layer_dict[layer_in].register_forward_hook(hook_in))\n",
    "            nx = torch.zeros([x.size()[0]]+self.input_size)\n",
    "\n",
    "        if not layer_out == \"output\":\n",
    "            handles.append(self.layer_dict[layer_out].register_forward_hook(hook_out))\n",
    "\n",
    "        if self.CUDA:\n",
    "            nx = nx.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            ny = self.model(nx)\n",
    "\n",
    "        #print(data_out)\n",
    "\n",
    "        if layer_out == \"output\":\n",
    "            data_out = ny\n",
    "        else:\n",
    "            data_out = data_out[0]\n",
    "\n",
    "        data_out = data_out.cpu()\n",
    "\n",
    "        for handle in handles:\n",
    "            handle.remove() \n",
    "\n",
    "        if self.non_negative:\n",
    "            data_out = torch.relu(data_out)\n",
    "\n",
    "        return data_out\n",
    "\n",
    "    def _batch_fn(self,x,layer_in = \"input\",layer_out = \"output\"):\n",
    "        #numpy in numpy out\n",
    "                \n",
    "        if type(x) == torch.Tensor or type(x) == np.ndarray:\n",
    "            x = self._to_tensor(x)\n",
    "\n",
    "            dataset = TensorDataset(x)\n",
    "            x = DataLoader(dataset,batch_size=self.batch_size)\n",
    "        \n",
    "\n",
    "        out = []\n",
    "\n",
    "        for nx in x:\n",
    "            nx = nx[0]\n",
    "            nx = self._switch_channel(nx,layer_in=layer_in,layer_out=layer_out,to_model=True)\n",
    "            out.append(self._fun(nx,layer_in,layer_out))\n",
    "\n",
    "        res = torch.cat(out,0)\n",
    "\n",
    "        res = self._switch_channel(res,layer_in=layer_in,layer_out=layer_out,to_model=False)\n",
    "        if self.numpy_out:\n",
    "            res = res.detach().numpy()\n",
    "\n",
    "\n",
    "        return res\n",
    "\n",
    "    def set_predict_target(self,predict_target):\n",
    "        self.predict_target = predict_target\n",
    "    \n",
    "    def get_feature(self,x,layer_name):\n",
    "        if layer_name not in self.layer_dict:\n",
    "            print (\"Target layer not exists\")\n",
    "            return None\n",
    "\n",
    "        out = self._batch_fn(x,layer_out = layer_name)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def feature_predict(self,feature,layer_name = None):\n",
    "\n",
    "        if layer_name not in self.layer_dict:\n",
    "            print (\"Target layer not exists\")\n",
    "            return None\n",
    "\n",
    "        out = self._batch_fn(feature,layer_in = layer_name)\n",
    "        if self.predict_target is not None:\n",
    "            out = out[:,self.predict_target]\n",
    "        return out        \n",
    "\n",
    "\n",
    "    def predict(self,x):\n",
    "\n",
    "        out = self._batch_fn(x)\n",
    "        if self.predict_target is not None:\n",
    "            out = out[:,self.predict_target]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "094896bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "#npdir = '/dataset/ILSVRC2012/nparray_train'\n",
    "#imdir = '/dataset/ILSVRC2012/ILSVRC2012_img_train'\n",
    "\n",
    "mean = [103.939, 116.779, 123.68]\n",
    "SIZE = [224,224]\n",
    "EPSILON = 1e-8\n",
    "\n",
    "class img_utils():\n",
    "\n",
    "    def __init__(self,\n",
    "                img_size = (224,224),\n",
    "                nchannels = 3,\n",
    "                img_format = \"channels_last\",\n",
    "                mode = None,\n",
    "                std = None,\n",
    "                mean = None):\n",
    "        self.img_format = img_format\n",
    "        self.nchannels = nchannels\n",
    "        self.fsize = list(img_size)\n",
    "        self.img_size = self.fsize + [self.nchannels]\n",
    "        #if img_format == 'channels_first':\n",
    "        #    self.img_size = [self.nchannels] + self.fsize\n",
    "        #else:\n",
    "        #    self.img_size = self.fsize + [self.nchannels]\n",
    "        \n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def deprocessing(self,x):\n",
    "        mode = self.mode\n",
    "        x = np.array(x)\n",
    "        X = x.copy()\n",
    "\n",
    "        if self.img_format == \"channels_first\":\n",
    "            if X.ndim == 3:\n",
    "                X = np.transpose(X,(1,2,0))\n",
    "            else:\n",
    "                X = np.transpose(X,(0,2,3,1))\n",
    "\n",
    "        if mode is None:\n",
    "            return X\n",
    "\n",
    "        if mode == \"tf\":\n",
    "            X +=1\n",
    "            X *=127.5\n",
    "            return X\n",
    "\n",
    "        if mode == \"torch\":\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if mode == 'caffe':\n",
    "            mean = [103.939, 116.779, 123.68]\n",
    "            std = None            \n",
    "\n",
    "        if mode == \"norm\":\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "\n",
    "\n",
    "        if std is not None:\n",
    "            X[..., 0] *= std[0]\n",
    "            X[..., 1] *= std[1]\n",
    "            X[..., 2] *= std[2]        \n",
    "        X[..., 0] += mean[0]\n",
    "        X[..., 1] += mean[1]\n",
    "        X[..., 2] += mean[2]   \n",
    "            \n",
    "        if mode == 'caffe':\n",
    "            # 'RGB'->'BGR'\n",
    "            X = X[..., ::-1]  \n",
    "        \n",
    "        if mode == \"torch\":\n",
    "            X *= 255\n",
    "        return X\n",
    "    \n",
    "    def resize_img(self,array, smooth = False):\n",
    "        fsize = self.fsize\n",
    "        size = array.shape\n",
    "        if smooth:            \n",
    "            tsize = list(size)\n",
    "            tsize[1] = fsize[0]\n",
    "            tsize[2] = fsize[1]\n",
    "            res = resize(array, tsize, order=1, mode='reflect',anti_aliasing=False)\n",
    "        else:\n",
    "            res = []\n",
    "            for i in range(size[0]):\n",
    "                temp = array[i]\n",
    "                temp = np.repeat(temp,fsize[0]//size[1],axis = 0)\n",
    "                temp = np.repeat(temp,fsize[1]//size[2],axis = 1)\n",
    "                res.append(temp)\n",
    "            res = np.array(res)\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def flatten(self,array):\n",
    "        size = array.shape\n",
    "        return array.reshape(-1,size[-1])  \n",
    "\n",
    "    def show_img(self,X,nrows=1,ncols=1,heatmaps = None,useColorBar = True, deprocessing = True, save_path = None):\n",
    "        X = np.array(X)\n",
    "        if not heatmaps is None:\n",
    "            heatmaps = np.array(heatmaps)\n",
    "        if len(X.shape)<4:\n",
    "            print (\"Dim should be 4\")\n",
    "            return \n",
    "\n",
    "        X = np.array(X)\n",
    "        if deprocessing:\n",
    "            X = self.deprocessing(X)\n",
    "\n",
    "        if (not X.min() == 0) or X.max()>1:\n",
    "            X = X - X.min()\n",
    "            X = X / X.max()\n",
    "\n",
    "        if X.shape[0] == 1:\n",
    "            X = np.squeeze(X)\n",
    "            X = np.expand_dims(X,axis=0)\n",
    "        else:\n",
    "            X = np.squeeze(X)\n",
    "\n",
    "        if self.nchannels == 1:\n",
    "            cmap = \"Greys\"\n",
    "        else:\n",
    "            cmap = \"viridis\"\n",
    "        \n",
    "        l = nrows*ncols\n",
    "        plt.figure(figsize=(5*ncols, 5*nrows))\n",
    "        for i in range(l):\n",
    "            plt.subplot(nrows, ncols, i+1)\n",
    "            plt.axis('off')\n",
    "            img = X[i]\n",
    "            img = np.clip(img,0,1)\n",
    "            plt.imshow(img,cmap = cmap)\n",
    "            if not heatmaps is None:\n",
    "                if not heatmaps[i] is None:\n",
    "                    heapmap = heatmaps[i]\n",
    "                    plt.imshow(heapmap, cmap='jet', alpha=0.5,interpolation = \"bilinear\")\n",
    "                    if useColorBar:\n",
    "                        plt.colorbar()\n",
    "\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()   \n",
    "        \n",
    "\n",
    "    def img_filter(self,x,h,threshold=0.5,background = 0.2,smooth = True, minmax = False):\n",
    "        x = x.copy()\n",
    "        h = h.copy()\n",
    "\n",
    "        if minmax:\n",
    "            h = h - h.min()\n",
    "\n",
    "        h = h * (h>0)\n",
    "        for i in range(h.shape[0]):\n",
    "            h[i] = h[i] / (h[i].max() + EPSILON)\n",
    "\n",
    "        h = (h - threshold) * (1/(1-threshold))\n",
    "        \n",
    "        #h = h * (h>0)\n",
    "\n",
    "        h = self.resize_img(h,smooth = smooth)\n",
    "        h = ((h>0).astype(\"float\")*(1-background) + background)\n",
    "        h_mask = np.repeat(h,self.nchannels).reshape(list(h.shape)+[-1])\n",
    "        if self.img_format == 'channels_first':\n",
    "            h_mask = np.transpose(h_mask,(0,3,1,2))\n",
    "        x = x * h_mask\n",
    "        \n",
    "        h = h - h.min()\n",
    "        h = h / (h.max() + EPSILON)\n",
    "        \n",
    "        return x,h\n",
    "\n",
    "\n",
    "    def contour_img(self,x,h,dpi = 100):\n",
    "        dpi = float(dpi)\n",
    "        size = x.shape\n",
    "        if x.max()>1:\n",
    "            x = x/x.max()\n",
    "        #fig = plt.figure(figsize=(size[1]/dpi,size[0]/dpi),dpi=dpi)\n",
    "        fig = plt.figure(figsize=(size[1]/dpi,size[0]/dpi),dpi=dpi*SIZE[0]/size[0])\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax)\n",
    "        xa = np.linspace(0, size[1]-1, size[1])\n",
    "        ya = np.linspace(0, size[0]-1, size[0])\n",
    "        X, Y = np.meshgrid(xa, ya)\n",
    "        if x.shape[-1] == 1:\n",
    "            x = np.squeeze(x)\n",
    "            ax.imshow(x,cmap = \"Greys\")\n",
    "        else:\n",
    "            ax.imshow(x)\n",
    "        ax.contour(X, Y, h,colors = 'r')\n",
    "        return fig\n",
    "\n",
    "    def res_ana(self,model,classesLoader,classNos,reducer, layer_name = \"conv5_block3_out\"):\n",
    "\n",
    "        w,b = model.model.layers[-1].get_weights()\n",
    "        V_ = reducer._reducer.components_\n",
    "        w_ = np.dot(V_,w)\n",
    "        ana = []\n",
    "        \n",
    "        for No in classNos:\n",
    "            target = No\n",
    "            tX, ty = classesLoader.load_val([No])\n",
    "            tX = np.concatenate(tX)\n",
    "            ty = np.concatenate(ty).astype(int)   \n",
    "            X = model.get_feature(tX,layer_name=layer_name).mean(axis = (1,2))\n",
    "            S = reducer.transform(X)\n",
    "            U = X-np.dot(S,V_)\n",
    "\n",
    "            C1 = np.dot(X,w[:,target])\n",
    "            C2 = np.dot(S,w_[:,target]) + np.dot(U,w[:,target])\n",
    "\n",
    "            C = np.dot(S,w_[:,target])\n",
    "            res = np.dot(U,w[:,target])\n",
    "\n",
    "            ana.append(np.array([C1,C2,C,res]))\n",
    "        return [np.array(ana),reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f97a00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 The Lucid Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Helper for using sklearn.decomposition on high-dimensional tensors.\n",
    "Provides ChannelReducer, a wrapper around sklearn.decomposition to help them\n",
    "apply to arbitrary rank tensors. It saves lots of annoying reshaping.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.cluster\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator \n",
    "\n",
    "ALGORITHM_NAMES = {}\n",
    "for name in dir(sklearn.decomposition):\n",
    "    obj = sklearn.decomposition.__getattribute__(name)\n",
    "    if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
    "        ALGORITHM_NAMES[name] = 'decomposition'\n",
    "for name in dir(sklearn.cluster):\n",
    "    obj = sklearn.cluster.__getattribute__(name)\n",
    "    if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
    "        ALGORITHM_NAMES[name] = 'cluster'\n",
    "\n",
    "\n",
    "class ChannelDecompositionReducer(object):\n",
    "\n",
    "    def __init__(self, n_components=3, reduction_alg=\"NMF\", **kwargs):\n",
    "\n",
    "        if not isinstance(n_components, int):\n",
    "            raise ValueError(\"n_components must be an int, not '%s'.\" % n_components)\n",
    "\n",
    "        # Defensively look up reduction_alg if it is a string and give useful errors.\n",
    "        algorithm_map = {}\n",
    "        for name in dir(sklearn.decomposition):\n",
    "            obj = sklearn.decomposition.__getattribute__(name)\n",
    "            if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
    "                algorithm_map[name] = obj\n",
    "        if isinstance(reduction_alg, str):\n",
    "            if reduction_alg in algorithm_map:\n",
    "                reduction_alg = algorithm_map[reduction_alg]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown dimensionality reduction method '%s'.\" % reduction_alg)\n",
    "\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self._reducer = reduction_alg(n_components=n_components, **kwargs)\n",
    "        self._is_fit = False\n",
    "\n",
    "    def _apply_flat(cls, f, acts):\n",
    "        orig_shape = acts.shape\n",
    "        acts_flat = acts.reshape([-1, acts.shape[-1]])\n",
    "        new_flat = f(acts_flat)\n",
    "        if not isinstance(new_flat, np.ndarray):\n",
    "            return new_flat\n",
    "        shape = list(orig_shape[:-1]) + [-1]\n",
    "        return new_flat.reshape(shape)\n",
    "\n",
    "    def fit(self, acts):\n",
    "        if hasattr(self._reducer,'partial_fit'):\n",
    "            res = self._apply_flat(self._reducer.partial_fit, acts)\n",
    "        else:\n",
    "            res = self._apply_flat(self._reducer.fit, acts)\n",
    "        self._is_fit = True\n",
    "        return res\n",
    "\n",
    "    def fit_transform(self, acts):\n",
    "        res = self._apply_flat(self._reducer.fit_transform, acts)\n",
    "        self._is_fit = True\n",
    "        return res\n",
    "\n",
    "    def transform(self, acts):\n",
    "        res = self._apply_flat(self._reducer.transform, acts)\n",
    "        return res\n",
    "\n",
    "    def inverse_transform(self, acts):\n",
    "        if hasattr(self._reducer,'inverse_transform'):\n",
    "            res = self._apply_flat(self._reducer.inverse_transform, acts)\n",
    "        else:\n",
    "            res = np.dot(acts,self._reducer.components_)\n",
    "        return res\n",
    "\n",
    "\n",
    "class ChannelClusterReducer(object):\n",
    "\n",
    "    def __init__(self, n_components=3, reduction_alg=\"KMeans\", **kwargs):\n",
    "\n",
    "\n",
    "        if not isinstance(n_components, int):\n",
    "            raise ValueError(\"n_components must be an int, not '%s'.\" % n_components)\n",
    "\n",
    "        # Defensively look up reduction_alg if it is a string and give useful errors.\n",
    "        algorithm_map = {}\n",
    "        for name in dir(sklearn.cluster):\n",
    "            obj = sklearn.cluster.__getattribute__(name)\n",
    "            if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
    "                algorithm_map[name] = obj\n",
    "        if isinstance(reduction_alg, str):\n",
    "            if reduction_alg in algorithm_map:\n",
    "                reduction_alg = algorithm_map[reduction_alg]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown dimensionality reduction method '%s'.\" % reduction_alg)\n",
    "\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self._reducer = reduction_alg(n_clusters=n_components, **kwargs)\n",
    "        self._is_fit = False\n",
    "\n",
    "    def _apply_flat(self, f, acts):\n",
    "        \"\"\"Utility for applying f to inner dimension of acts.\n",
    "        Flattens acts into a 2D tensor, applies f, then unflattens so that all\n",
    "        dimesnions except innermost are unchanged.\n",
    "        \"\"\"\n",
    "        orig_shape = acts.shape\n",
    "        acts_flat = acts.reshape([-1, acts.shape[-1]])\n",
    "        new_flat = f(acts_flat)\n",
    "        if not isinstance(new_flat, np.ndarray):\n",
    "            return new_flat\n",
    "        shape = list(orig_shape[:-1]) + [-1]\n",
    "        new_flat = new_flat.reshape(shape)\n",
    "\n",
    "\n",
    "        if new_flat.shape[-1] == 1:\n",
    "            new_flat = new_flat.reshape(-1)\n",
    "            t_flat = np.zeros([new_flat.shape[0],self.n_components])\n",
    "            t_flat[np.arange(new_flat.shape[0]),new_flat] = 1\n",
    "            new_flat = t_flat.reshape(shape)\n",
    "\n",
    "        return new_flat\n",
    "\n",
    "    def fit(self, acts):\n",
    "        if hasattr(self._reducer,'partial_fit'):\n",
    "            res = self._apply_flat(self._reducer.partial_fit, acts)\n",
    "        else:\n",
    "            res = self._apply_flat(self._reducer.fit, acts)\n",
    "        self._reducer.components_ = self._reducer.cluster_centers_\n",
    "        self._is_fit = True\n",
    "        return res\n",
    "\n",
    "    def fit_predict(self, acts):\n",
    "        res = self._apply_flat(self._reducer.fit_predict, acts)\n",
    "        self._reducer.components_ = self._reducer.cluster_centers_\n",
    "        self._is_fit = True\n",
    "        return res\n",
    "\n",
    "    def transform(self, acts):\n",
    "        res = self._apply_flat(self._reducer.predict, acts)\n",
    "        return res\n",
    "\n",
    "    def inverse_transform(self, acts):\n",
    "        res = np.dot(acts,self._reducer.components_)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2044af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "FONT_SIZE = 30\n",
    "#CALC_LIMIT = 3e4\n",
    "CALC_LIMIT = 1e8\n",
    "#CALC_LIMIT = 1e9\n",
    "TRAIN_LIMIT = 50\n",
    "REDUCER_PATH = \"reducer/resnet50\"\n",
    "USE_TRAINED_REDUCER = False\n",
    "ESTIMATE_NUM = 10\n",
    "\n",
    "class Explainer():\n",
    "    def __init__(self,\n",
    "                 title = \"\",\n",
    "                 layer_name = \"\",\n",
    "                 class_names = None,\n",
    "                 utils = None,\n",
    "                 keep_feature_images = True,\n",
    "                 useMean = True,\n",
    "                 reducer_type = \"NMF\",\n",
    "                 n_components = 10,\n",
    "                 featuretopk = 20,\n",
    "                 featureimgtopk = 5,\n",
    "                 epsilon = 1e-4):\n",
    "        self.title = title\n",
    "        self.layer_name = layer_name\n",
    "        self.class_names = class_names\n",
    "        self.class_nos = len(class_names) if class_names is not None else 0\n",
    "\n",
    "        self.keep_feature_images = keep_feature_images\n",
    "        self.useMean = useMean\n",
    "        self.reducer_type = reducer_type\n",
    "        self.featuretopk = featuretopk\n",
    "        self.featureimgtopk = featureimgtopk #number of images for a feature\n",
    "        self.n_components = n_components\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.utils = utils\n",
    "\n",
    "        self.reducer = None\n",
    "        self.feature_distribution = None\n",
    "\n",
    "        self.feature_base = []\n",
    "        self.features = {}\n",
    "\n",
    "        self.exp_location = Path('Explainers')\n",
    "\n",
    "        self.font = FONT_SIZE\n",
    "        \n",
    "    def load(self):\n",
    "        title = self.title\n",
    "        with open(self.exp_location / title / (title+\".pickle\"),\"rb\") as f:\n",
    "            tdict = pickle.load(f)\n",
    "            self.__dict__.update(tdict)\n",
    "            \n",
    "    def save(self):\n",
    "        if not os.path.exists(self.exp_location):\n",
    "            os.mkdir(self.exp_location)\n",
    "        title = self.title\n",
    "        if not os.path.exists(self.exp_location / title):\n",
    "            os.mkdir(self.exp_location / title)\n",
    "        with open(self.exp_location / title / (title+\".pickle\"),\"wb\") as f:\n",
    "            pickle.dump(self.__dict__,f)\n",
    "\n",
    "    def train_model(self,model,loaders):\n",
    "        self._train_reducer(model,loaders)\n",
    "        self._estimate_weight(model,loaders)\n",
    "\n",
    "    def _train_reducer(self,model,loaders):\n",
    "\n",
    "        print (\"Training reducer:\")\n",
    "\n",
    "        if self.reducer is None:\n",
    "            if not self.reducer_type in ALGORITHM_NAMES:\n",
    "                print ('reducer not exist')\n",
    "                return \n",
    "\n",
    "            if ALGORITHM_NAMES[self.reducer_type] == 'decomposition':\n",
    "                self.reducer = ChannelDecompositionReducer(n_components = self.n_components,reduction_alg = self.reducer_type)\n",
    "            else:\n",
    "                self.reducer = ChannelClusterReducer(n_components = self.n_components,reduction_alg = self.reducer_type)\n",
    "        \n",
    "        X_features = []\n",
    "        for loader in loaders:\n",
    "            X_features.append(model.get_feature(loader,self.layer_name))\n",
    "        print ('1/5 Featuer maps gathered.')\n",
    "\n",
    "        if not self.reducer._is_fit:\n",
    "            nX_feature = np.concatenate(X_features)\n",
    "            total = np.product(nX_feature.shape)\n",
    "            l = nX_feature.shape[0]\n",
    "            if total > CALC_LIMIT:\n",
    "                p = CALC_LIMIT / total\n",
    "                print (\"dataset too big, train with {:.2f} instances\".format(p))\n",
    "                idx = np.random.choice(l,int(l*p),replace = False)\n",
    "                nX_feature = nX_feature[idx]\n",
    "\n",
    "            print (\"loading complete, with size of {}\".format(nX_feature.shape))\n",
    "            start_time = time.time()\n",
    "            nX = self.reducer.fit_transform(nX_feature)\n",
    "\n",
    "            print (\"2/5 Reducer trained, spent {} s.\".format(time.time()-start_time))\n",
    "        \n",
    "\n",
    "        self.cavs = self.reducer._reducer.components_\n",
    "        nX = nX.mean(axis = (1,2))\n",
    "        self.feature_distribution = {\"overall\":[(nX[:,i].mean(),nX[:,i].std(),nX[:,i].min(),nX[:,i].max()) for i in range(self.n_components)]}\n",
    "\n",
    "        reX = []\n",
    "        self.feature_distribution['classes'] = []\n",
    "        for X_feature in X_features:\n",
    "            t_feature = self.reducer.transform(X_feature)\n",
    "            pred_feature = self._feature_filter(t_feature)\n",
    "            self.feature_distribution['classes'].append([pred_feature.mean(axis=0),pred_feature.std(axis=0),pred_feature.min(axis=0),pred_feature.max(axis=0)])\n",
    "            reX.append(self.reducer.inverse_transform(t_feature))\n",
    "\n",
    "        err = []\n",
    "        for i in range(len(self.class_names)):\n",
    "            res_true = model.feature_predict(X_features[i],layer_name=self.layer_name)[:,i] #\n",
    "            res_recon = model.feature_predict(reX[i],layer_name=self.layer_name)[:,i] #\n",
    "            err.append(abs(res_true-res_recon).mean(axis=0) / (abs(res_true.mean(axis=0))+self.epsilon))\n",
    "\n",
    "\n",
    "        self.reducer_err = np.array(err)\n",
    "        if type(self.reducer_err) is not np.ndarray:\n",
    "            self.reducer_err = np.array([self.reducer_err])\n",
    "\n",
    "        print (\"3/5 Error estimated, fidelity: {}.\".format(self.reducer_err))\n",
    "\n",
    "        return self.reducer_err\n",
    "\n",
    "    def _estimate_weight(self,model,loaders):\n",
    "        if self.reducer is None:\n",
    "            return\n",
    "\n",
    "        X_features = []\n",
    "\n",
    "        for loader in loaders:\n",
    "            X_features.append(model.get_feature(loader,self.layer_name)[:ESTIMATE_NUM])\n",
    "        X_feature = np.concatenate(X_features)\n",
    "\n",
    "        print ('4/5 Weight estimator initialized.')\n",
    "        \n",
    "        self.test_weight = []\n",
    "        for i in range(self.n_components):\n",
    "            cav = self.cavs[i,:]\n",
    "\n",
    "            res1 =  model.feature_predict(X_feature - self.epsilon * cav,layer_name=self.layer_name)\n",
    "            res2 =  model.feature_predict(X_feature + self.epsilon * cav,layer_name=self.layer_name)\n",
    "\n",
    "            res_dif = res2 - res1\n",
    "            dif = res_dif.mean(axis=0) / (2 * self.epsilon)\n",
    "            if type(dif) is not np.ndarray:\n",
    "                dif = np.array([dif])\n",
    "            self.test_weight.append(dif)\n",
    "        \n",
    "        print ('5/5 Weight estimated.')\n",
    "\n",
    "        self.test_weight = np.array(self.test_weight)\n",
    "\n",
    "    def generate_features(self,model,loaders):\n",
    "        self._visulize_features(model,loaders)\n",
    "        self._save_features()\n",
    "        if self.keep_feature_images == False:\n",
    "            self.features = {}\n",
    "        return\n",
    "\n",
    "    def _feature_filter(self,featureMaps,threshold = None):\n",
    "    #filter feature map to feature value with threshold for target value\n",
    "        if self.useMean:\n",
    "            res = featureMaps.mean(axis = (1,2))\n",
    "        else:\n",
    "            res = featureMaps.max(axis=(1,2))\n",
    "        if threshold is not None:\n",
    "            res = - abs(res - threshold) \n",
    "        return res\n",
    "   \n",
    "    def _update_feature_dict(self,x,h,nx,nh,threshold = None):\n",
    "\n",
    "        if type(x) == type(None):\n",
    "            return nx,nh\n",
    "        else:\n",
    "            x = np.concatenate([x,nx])\n",
    "            h = np.concatenate([h,nh])\n",
    "\n",
    "            nidx = self._feature_filter(h,threshold = threshold).argsort()[-self.featureimgtopk:]\n",
    "            x = x[nidx,...]\n",
    "            h = h[nidx,...]\n",
    "            return x,h\n",
    "\n",
    "    def save_concept_crops(self, concept_id, images, heatmaps, save_dir, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Save cropped image regions corresponding to a concept's activation.\n",
    "        Args:\n",
    "            concept_id (int): Concept index.\n",
    "            images (numpy.ndarray): Shape [N, H, W, C], in [0,1] or normalized.\n",
    "            heatmaps (numpy.ndarray): Shape [N, H_small, W_small], one per image.\n",
    "            save_dir (Path): Directory where crops will be saved.\n",
    "            threshold (float): Threshold for activation masking.\n",
    "        \"\"\"\n",
    "\n",
    "        concept_dir = save_dir / f\"concept_{concept_id}\"\n",
    "        concept_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            heatmap = heatmaps[i]\n",
    "\n",
    "            # Resize heatmap to match image resolution\n",
    "            heatmap_resized = self.utils.resize_img(np.expand_dims(heatmap, 0), smooth=True)[0]  # [H, W]\n",
    "\n",
    "            # Normalize heatmap to [0, 1] just in case\n",
    "            heatmap_resized -= heatmap_resized.min()\n",
    "            heatmap_resized /= (heatmap_resized.max() + 1e-8)\n",
    "\n",
    "            # Threshold to get binary mask\n",
    "            binary_mask = (heatmap_resized > threshold).astype(np.uint8)\n",
    "\n",
    "            # Find bounding box\n",
    "            ys, xs = np.where(binary_mask == 1)\n",
    "            if len(xs) == 0 or len(ys) == 0:\n",
    "                continue  # skip images with no activated region\n",
    "\n",
    "            x_min, x_max = xs.min(), xs.max()\n",
    "            y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "            # Sanity clamp (optional)\n",
    "            if x_max - x_min < 5 or y_max - y_min < 5:\n",
    "                continue  # skip too small crops\n",
    "\n",
    "            # Clip coordinates to image bounds\n",
    "            h_img, w_img = image.shape[:2]\n",
    "            x_min = max(x_min, 0)\n",
    "            y_min = max(y_min, 0)\n",
    "            x_max = min(x_max, w_img - 1)\n",
    "            y_max = min(y_max, h_img - 1)\n",
    "\n",
    "            # Crop and save\n",
    "            crop = image[y_min:y_max+1, x_min:x_max+1, :]  # [H, W, C]\n",
    "            crop = np.clip(crop, 0, 1)  # Make sure values are in [0, 1]\n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(crop, torch.Tensor):\n",
    "                crop = crop.detach().cpu().numpy()\n",
    "\n",
    "            # Convert to uint8 image and save\n",
    "            crop_img = Image.fromarray((crop * 255).astype(np.uint8))\n",
    "            crop_img.save(concept_dir / f\"crop_{i}.png\")\n",
    "        \n",
    "    def _visulize_features(self,model,loaders, featureIdx = None, inter_dict = None):\n",
    "        featuretopk = min(self.featuretopk, self.n_components)\n",
    "\n",
    "        imgTopk = self.featureimgtopk\n",
    "        if featureIdx is None:\n",
    "            featureIdx = []\n",
    "            tidx = []\n",
    "            w = self.test_weight\n",
    "            for i,_ in enumerate(self.class_names):\n",
    "                tw = w[:,i]\n",
    "                tidx += tw.argsort()[::-1][:featuretopk].tolist()\n",
    "            featureIdx += list(set(tidx))                    \n",
    "\n",
    "        nowIdx = set(self.features.keys())\n",
    "        featureIdx = list(set(featureIdx) - nowIdx)\n",
    "        featureIdx.sort()\n",
    "\n",
    "        if len(featureIdx) == 0:\n",
    "            print (\"All feature gathered\")\n",
    "            return\n",
    "\n",
    "        print (\"visulizing features:\")\n",
    "        print (featureIdx)\n",
    "\n",
    "        features = {}\n",
    "        for No in featureIdx:\n",
    "            features[No] = [None,None]\n",
    "            \n",
    "        if inter_dict is not None:\n",
    "            for k in inter_dict.keys():\n",
    "                inter_dict[k] = [[None,None] for No in featureIdx]\n",
    "        \n",
    "        print (\"loading training data\")\n",
    "        for i,loader in enumerate(loaders):\n",
    "            \n",
    "            for X in loader:\n",
    "                X = X[0]\n",
    "                featureMaps = self.reducer.transform(model.get_feature(X,self.layer_name))\n",
    "                \n",
    "                X_feature = self._feature_filter(featureMaps)\n",
    "\n",
    "                for No in featureIdx:\n",
    "                    samples,heatmap = features[No]\n",
    "                    idx = X_feature[:,No].argsort()[-imgTopk:]\n",
    "                    \n",
    "                    nheatmap = featureMaps[idx,:,:,No]\n",
    "                    nsamples = X[idx,...]\n",
    "                    \n",
    "                    samples,heatmap = self._update_feature_dict(samples,heatmap,nsamples,nheatmap)\n",
    "                    \n",
    "                    features[No] = [samples,heatmap]\n",
    "                    self.save_concept_crops(\n",
    "                                        concept_id=No,\n",
    "                                        images=samples,\n",
    "                                        heatmaps=heatmap,\n",
    "                                        save_dir=self.exp_location / self.title / \"concept_crops\",\n",
    "                                        threshold=0.5\n",
    "                                    )\n",
    "                    \n",
    "                    if inter_dict is not None:\n",
    "                        for k in inter_dict.keys():\n",
    "                            vmin = self.feature_distribution['overall'][No][2]\n",
    "                            vmax = self.feature_distribution['overall'][No][3]\n",
    "                            temp_v = (vmax - vmin) * k + vmin\n",
    "                            inter_dict[k][No] = self._update_feature_dict(inter_dict[k][No][0],inter_dict[k][No][1],X,featureMaps[:,:,:,No],threshold = temp_v)\n",
    "                        \n",
    "                    \n",
    "            print (\"Done with class: {}, {}/{}\".format(self.class_names[i],i+1,len(loaders)))\n",
    "        # create repeat prototypes in case lack of samples\n",
    "        for no,(x,h) in features.items():\n",
    "            idx = h.mean(axis = (1,2)).argmax()\n",
    "            for i in range(h.shape[0]):\n",
    "                if h[i].max() == 0:\n",
    "                    x[i] = x[idx]\n",
    "                    h[i] = h[idx]\n",
    "        \n",
    "        self.features.update(features)\n",
    "        self.save()\n",
    "        return inter_dict\n",
    "\n",
    "    def _save_features(self,threshold=0.5,background = 0.2,smooth = True):\n",
    "        feature_path = self.exp_location / self.title / \"feature_imgs\"\n",
    "        #utils = self.utils\n",
    "\n",
    "        if not os.path.exists(feature_path):\n",
    "            os.mkdir(feature_path)\n",
    "\n",
    "        for idx in self.features.keys(): \n",
    "\n",
    "            x,h = self.features[idx]\n",
    "            #x = self.gen_masked_imgs(x,h,threshold=threshold,background = background,smooth = smooth)\n",
    "            minmax = False\n",
    "            if self.reducer_type == 'PCA':\n",
    "                minmax = True\n",
    "            x,h = self.utils.img_filter(x,h,threshold=threshold,background = background,smooth = smooth,minmax = minmax)\n",
    "            \n",
    "            nsize = self.utils.img_size.copy()\n",
    "            nsize[1] = nsize[1]* self.featureimgtopk\n",
    "            nimg = np.zeros(nsize)\n",
    "            nh = np.zeros(nsize[:-1])\n",
    "            for i in range(x.shape[0]):\n",
    "                timg = self.utils.deprocessing(x[i])\n",
    "                if timg.max()>1:\n",
    "                    timg = timg / 255.0\n",
    "                    timg = abs(timg)\n",
    "                timg = np.clip(timg,0,1)\n",
    "                nimg[:,i*self.utils.img_size[1]:(i+1)*self.utils.img_size[1],:] = timg\n",
    "                nh[:,i*self.utils.img_size[1]:(i+1)*self.utils.img_size[1]] = h[i]\n",
    "            fig = self.utils.contour_img(nimg,nh)\n",
    "            fig.savefig(feature_path / (str(idx)+\".jpg\"),bbox_inches='tight',pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "    def global_explanations(self):        \n",
    "        title = self.title\n",
    "        fpath = (self.exp_location / self.title / \"feature_imgs\").absolute()\n",
    "        feature_topk = min(self.featuretopk,self.n_components)\n",
    "        feature_weight = self.test_weight\n",
    "        class_names = self.class_names\n",
    "        Nos = range(self.class_nos)\n",
    "\n",
    "        font = self.font\n",
    "        \n",
    "        def LR_graph(wlist,No):\n",
    "            def node_string(count,fidx,w,No):\n",
    "                nodestr = \"\"\n",
    "                nodestr += \"{} [label=< <table border=\\\"0\\\">\".format(count)\n",
    "\n",
    "                nodestr+=\"<tr>\"\n",
    "                nodestr+=\"<td><img src= \\\"{}\\\" /></td>\".format(str(fpath / (\"{}.jpg\".format(fidx)))) \n",
    "                nodestr+=\"</tr>\"\n",
    "\n",
    "\n",
    "                #nodestr +=\"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> ClassName: {} </FONT></td></tr>\".format(font,classes.No2Name[No])\n",
    "                nodestr +=\"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> FeatureRank: {} </FONT></td></tr>\".format(font,count)\n",
    "\n",
    "                nodestr +=\"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> Feature: {}, Weight: {:.3f} </FONT></td></tr>\".format(font,fidx,w)\n",
    "\n",
    "                nodestr += \"</table>  >];\\n\" \n",
    "                return nodestr\n",
    "\n",
    "            resstr = \"digraph Tree {node [shape=box] ;rankdir = LR;\\n\"\n",
    "\n",
    "\n",
    "            count = len(wlist)\n",
    "            for k,v in wlist:\n",
    "                resstr+=node_string(count,k,v,No)\n",
    "                count-=1\n",
    "            \n",
    "            resstr += \"0 [label=< <table border=\\\"0\\\">\" \n",
    "            resstr += \"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> ClassName: {} </FONT></td></tr>\" .format(font,class_names[No])\n",
    "            resstr += \"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> Fidelity error: {:.3f} % </FONT></td></tr>\" .format(font,self.reducer_err[No]*100)\n",
    "            resstr += \"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> First {} features out of {} </FONT></td></tr>\" .format(font,feature_topk,self.n_components)\n",
    "            resstr += \"</table>  >];\\n\"\n",
    "            \n",
    "\n",
    "            resstr += \"}\"\n",
    "\n",
    "            return resstr\n",
    "\n",
    "        if not os.path.exists(self.exp_location / title / \"GE\"):\n",
    "            os.mkdir(self.exp_location / title / \"GE\")\n",
    "                    \n",
    "        print (\"Generate explanations with fullset condition\")\n",
    "\n",
    "        for i in Nos:\n",
    "            wlist = [(j,feature_weight[j][i]) for j in feature_weight[:,i].argsort()[-feature_topk:]]\n",
    "            graph = pydotplus.graph_from_dot_data(LR_graph(wlist,i))  \n",
    "            graph.write_jpg(str(self.exp_location / title / \"GE\" / (\"{}.jpg\".format(class_names[i]))))\n",
    " \n",
    "    def local_explanations(self,x,model,background = 0.2,name = None,with_total = True,display_value = True):\n",
    "        utils = self.utils\n",
    "        font = self.font\n",
    "        featuretopk = min(self.featuretopk,self.n_components)\n",
    "\n",
    "        target_classes = list(range(self.class_nos))\n",
    "        w = self.test_weight\n",
    "\n",
    "        pred = model.predict(x)[0][target_classes]\n",
    "        \n",
    "        fpath = self.exp_location / self.title / \"explanations\"\n",
    "\n",
    "        if not os.path.exists(fpath):\n",
    "            os.mkdir(fpath)\n",
    "\n",
    "        afpath = fpath / \"all\"\n",
    "\n",
    "        if not os.path.exists(afpath):\n",
    "            os.mkdir(afpath)\n",
    "\n",
    "\n",
    "        if name is not None:\n",
    "            fpath = fpath / name\n",
    "            if not os.path.exists(fpath):\n",
    "                os.mkdir(fpath)\n",
    "            else:\n",
    "                print (\"Folder exists\")\n",
    "                return \n",
    "        else:\n",
    "            count = 0\n",
    "            while os.path.exists(fpath / str(count)):\n",
    "                count+=1\n",
    "            fpath = fpath / str(count)\n",
    "            os.mkdir(fpath)\n",
    "            name = str(count)\n",
    "\n",
    "\n",
    "        if self.reducer is not None:\n",
    "            h = self.reducer.transform(model.get_feature(x,self.layer_name))[0]\n",
    "        else:\n",
    "            h = model.get_feature(x,self.layer_name)[0]\n",
    "\n",
    "        feature_idx = []\n",
    "        for cidx in target_classes:\n",
    "            tw = w[:,cidx]\n",
    "            tw_idx = tw.argsort()[::-1][:featuretopk]\n",
    "            feature_idx.append(tw_idx)\n",
    "        feature_idx = list(set(np.concatenate(feature_idx).tolist()))\n",
    "\n",
    "        \n",
    "\n",
    "        for k in feature_idx:\n",
    "            \n",
    "            minmax = False\n",
    "            if self.reducer_type == \"PCA\":\n",
    "                minmax = True\n",
    "\n",
    "            x1,h1 = utils.img_filter(x,np.array([h[:,:,k]]),background=background,minmax = minmax)\n",
    "            x1 = utils.deprocessing(x1)\n",
    "            x1 = x1 / x1.max()\n",
    "            x1 = abs(x1)\n",
    "            fig = utils.contour_img(x1[0],h1[0])\n",
    "            fig.savefig(fpath / (\"feature_{}.jpg\".format(k)))\n",
    "            plt.close()\n",
    "\n",
    "        fpath = fpath.absolute()\n",
    "        gpath = self.exp_location.absolute() / self.title / 'feature_imgs'\n",
    "        def node_string(fidx,score,weight):\n",
    "            \n",
    "            \n",
    "            nodestr = \"\"\n",
    "            nodestr += \"<table border=\\\"0\\\">\\n\"\n",
    "            nodestr+=\"<tr>\"\n",
    "            nodestr+=\"<td><img src= \\\"{}\\\" /></td>\".format(str(fpath / (\"feature_{}.jpg\".format(fidx))))\n",
    "            nodestr+=\"<td><img src= \\\"{}\\\" /></td>\".format(str(gpath / (\"{}.jpg\".format(fidx))))\n",
    "            nodestr+=\"</tr>\\n\"\n",
    "            if display_value:\n",
    "                nodestr +=\"<tr><td colspan=\\\"2\\\"><FONT POINT-SIZE=\\\"{}\\\"> ClassName: {}, Feature: {}</FONT></td></tr>\\n\".format(font,self.class_names[cidx],fidx)\n",
    "                nodestr +=\"<tr><td colspan=\\\"2\\\"><FONT POINT-SIZE=\\\"{}\\\"> Similarity: {:.3f}, Weight: {:.3f}, Contribution: {:.3f}</FONT></td></tr> \\n\".format(font,score,weight,score*weight)\n",
    "            nodestr += \"</table>  \\n\" \n",
    "            return nodestr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        s = h.mean(axis = (0,1))\n",
    "        for cidx in target_classes:\n",
    "            tw = w[:,cidx]\n",
    "            tw_idx = tw.argsort()[::-1][:featuretopk] \n",
    "            \n",
    "            total = 0\n",
    "\n",
    "            resstr = \"digraph Tree {node [shape=plaintext] ;\\n\"\n",
    "            resstr += \"1 [label=< \\n<table border=\\\"0\\\"> \\n\"\n",
    "            for fidx in tw_idx:\n",
    "                resstr+=\"<tr><td>\\n\"\n",
    "                    \n",
    "                resstr+=node_string(fidx,s[fidx],tw[fidx])\n",
    "                total+=s[fidx]*tw[fidx]\n",
    "                    \n",
    "                resstr+=\"</td></tr>\\n\"\n",
    "\n",
    "            if with_total:\n",
    "                resstr +=\"<tr><td><FONT POINT-SIZE=\\\"{}\\\"> Total Conrtibution: {:.3f}, Prediction: {:.3f}</FONT></td></tr> \\n\".format(font,total,pred[cidx])\n",
    "            resstr += \"</table> \\n >];\\n\"\n",
    "            resstr += \"}\"\n",
    "\n",
    "            graph = pydotplus.graph_from_dot_data(resstr)  \n",
    "            graph.write_jpg(str(fpath / (\"explanation_{}.jpg\".format(cidx))))\n",
    "            graph.write_jpg(str(afpath / (\"{}_{}.jpg\".format(name,cidx))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dea16d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explainer(paras,model):\n",
    "  classes_names = paras['classes_names']\n",
    "  target_classes = paras['target_classes']\n",
    "  n_components = paras['n_components']\n",
    "  layer_name = paras['layer_name']\n",
    "  title = paras['title']\n",
    "  overwrite = paras['overwrite']\n",
    "\n",
    "  if overwrite:\n",
    "    try:\n",
    "        shutil.rmtree('Explainers/'+title)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "  model.set_predict_target(target_classes)\n",
    "  loaders = get_loaders(target_classes)\n",
    "  if os.path.exists(Path('Explainers')/title):\n",
    "    Exp = Explainer(title = title)\n",
    "    Exp.load()\n",
    "    print ('model loaded')\n",
    "  else:\n",
    "\n",
    "    # create an Explainer\n",
    "    Exp = Explainer(title = title,\n",
    "                    layer_name = layer_name,\n",
    "                    class_names = classes_names,\n",
    "                    utils = img_utils(mode = \"torch\"),\n",
    "                    n_components = n_components,\n",
    "                    reducer_type = \"NMF\"\n",
    "                  )\n",
    "\n",
    "    # train reducer based on target classes\n",
    "    Exp.train_model(model,loaders)\n",
    "    # generate features\n",
    "    Exp.generate_features(model, loaders)\n",
    "    # generate global explanations\n",
    "    Exp.global_explanations()\n",
    "    # save the explainer, use load to load it with the same title\n",
    "    Exp.save()\n",
    "\n",
    "  return Exp, loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89afefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PytorchModelWrapper(model,batch_size=8,predict_target=paras['target_classes'],input_channel_first = False,model_channel_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632bc608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reducer:\n",
      "1/5 Featuer maps gathered.\n",
      "loading complete, with size of (888, 7, 7, 512)\n",
      "2/5 Reducer trained, spent 13.558522701263428 s.\n",
      "3/5 Error estimated, fidelity: [0.06096106].\n",
      "4/5 Weight estimator initialized.\n",
      "5/5 Weight estimated.\n",
      "visulizing features:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]\n",
      "loading training data\n",
      "Done with class: church, 1/1\n",
      "Generate explanations with fullset condition\n"
     ]
    }
   ],
   "source": [
    "e, loaders= get_explainer(paras,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b41a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ice_concepts_and_metrics(exp: Explainer, model: PytorchModelWrapper, loaders):\n",
    "    \"\"\"\n",
    "    Extract concept basis W, activation representations U, and importance scores for ICE explanations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Step 1: Collect all features from loaders\n",
    "    print(\"Extracting features from loader...\")\n",
    "    all_features = []\n",
    "    for loader in loaders:\n",
    "        features = model.get_feature(loader, exp.layer_name)  # [N, H, W, C]\n",
    "        all_features.append(features)\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "    # Step 2: Transform into concept space using fitted reducer\n",
    "    assert exp.reducer._is_fit, \"Reducer is not fitted.\"\n",
    "    U = exp.reducer.transform(all_features)  # [N, H, W, K]\n",
    "\n",
    "    # Step 3: Collect concept basis vectors (W) and importance\n",
    "    W = exp.reducer._reducer.components_  # [K, D]\n",
    "    importance = np.mean(np.abs(exp.test_weight), axis=1)  # [K]\n",
    "\n",
    "    print(f\"Concepts extracted: U shape = {U.shape}, W shape = {W.shape}, importance = {importance.shape}\")\n",
    "\n",
    "    return U, W, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078675db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4515c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from loader...\n",
      "Concepts extracted: U shape = (888, 7, 7, 25), W shape = (25, 512), importance = (25,)\n"
     ]
    }
   ],
   "source": [
    "U, W, importances = extract_ice_concepts_and_metrics(e, model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba79070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UW = U@W\n",
    "UW = torch.tensor(UW, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc04c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "UW_reconstructed = UW.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b74c8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after NMF is: tensor(0.9899, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model = model.eval().to(device)\n",
    "g = nn.Sequential(*(list(model.children())[:-2]))  # All layers except the last two\n",
    "h = lambda x: model.fc(torch.mean(x, (2, 3))) \n",
    "h_2d = lambda x: model.fc(x)\n",
    "pred  = h(UW_reconstructed)\n",
    "c= torch.argmax(pred, dim=-1)\n",
    "accuracy = torch.sum(c==target_class)/len(c)\n",
    "print('accuracy after NMF is:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef193a0",
   "metadata": {},
   "source": [
    "### Complexity with C-Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b300ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini index\n",
    "def compute_gini_index(concept_importance):\n",
    "    if len(concept_importance.shape) != 1:\n",
    "        raise ValueError(\"Input concept_importance must be a 1D tensor.\")\n",
    "\n",
    "    sorted_importance = torch.sort(concept_importance)[0]\n",
    "    n = sorted_importance.size(0)\n",
    "    mean_importance = torch.mean(sorted_importance)\n",
    "\n",
    "    if mean_importance == 0:\n",
    "        return 0.0  # Gini index is 0 if all values are zero\n",
    "    cumulative_indices = torch.arange(1, n + 1, dtype=torch.float32, device=concept_importance.device)\n",
    "    numerator = torch.sum((2 * cumulative_indices - n - 1) * sorted_importance)\n",
    "    denominator = n * torch.sum(sorted_importance)\n",
    "\n",
    "    gini_index = numerator / denominator\n",
    "    return gini_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "964b8a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5211431980133057"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gini_index(torch.tensor(importances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba31ee",
   "metadata": {},
   "source": [
    "### Faithfulness using C-Ins and C-Del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf3439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_insertion(U, W, importances, h, imagenet_class):\n",
    "    U = torch.tensor(U, device='cuda')\n",
    "    W = torch.tensor(W, device='cuda')\n",
    "    importances = torch.tensor(importances, device='cuda')\n",
    "    sorted_indices = torch.argsort(importances, descending=True)\n",
    "    \n",
    "    u_insert = torch.zeros_like(U).to('cuda')\n",
    "    results = {}\n",
    "    \n",
    "    def predict(u):\n",
    "        uw = u @ W\n",
    "        uw = uw.permute(0, 3, 1, 2)\n",
    "        return h(uw)\n",
    "    \n",
    "    pred = predict(u_insert)\n",
    "    acc = (pred.argmax(dim=1) == imagenet_class).float().mean().item()\n",
    "    results[0] = acc\n",
    "    \n",
    "    for i in range(1, W.shape[0] + 1):\n",
    "        u_insert[..., sorted_indices[:i]] = U[..., sorted_indices[:i]]\n",
    "        pred = predict(u_insert)\n",
    "        acc = (pred.argmax(dim=1) == imagenet_class).float().mean().item()\n",
    "        results[i] = acc\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "199789ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_deletion(U, W, importances, h, imagenet_class):\n",
    "    U = torch.tensor(U, device='cuda')\n",
    "    W = torch.tensor(W, device='cuda')\n",
    "    importances = torch.tensor(importances, device='cuda')\n",
    "    sorted_indices = torch.argsort(importances, descending=True)\n",
    "    \n",
    "    results = {}\n",
    "    def predict(u):\n",
    "        uw = u @ W\n",
    "        uw = uw.permute(0, 3, 1, 2)\n",
    "        return h(uw)\n",
    "    \n",
    "    pred = predict(U)\n",
    "    acc = (pred.argmax(dim=1) == imagenet_class).float().mean().item()\n",
    "    results[0] = acc\n",
    "    \n",
    "    for i in range(1, W.shape[0] + 1):\n",
    "        u_mod = U.clone()\n",
    "        u_mod[..., sorted_indices[:i]] = 0\n",
    "        pred = predict(u_mod)\n",
    "        acc = (pred.argmax(dim=1) == imagenet_class).float().mean().item()\n",
    "        results[i] = acc\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3610c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_insertion_auc(insertion_scores):\n",
    "    x = np.array(list(insertion_scores.keys()))\n",
    "    y = np.array(list(insertion_scores.values()))\n",
    "    return np.trapz(y, x) / (x[-1] - x[0])\n",
    "\n",
    "def compute_deletion_score(deletion_scores):\n",
    "    x = np.array(list(deletion_scores.keys()))\n",
    "    y = np.array(list(deletion_scores.values()))\n",
    "    return (y[0] - y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fd2151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model = model.eval().to(device)\n",
    "g = nn.Sequential(*(list(model.children())[:-2]))  # All layers except the last two\n",
    "h = lambda x: model.fc(torch.mean(x, (2, 3))) \n",
    "h_2d = lambda x: model.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f1bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion AUC (C_Ins): 0.8861937177181244\n",
      "Deletion AUC (C_Del): 0.5872314721614552\n"
     ]
    }
   ],
   "source": [
    "insertion_scores = concept_insertion(U, W, importances, h, imagenet_class=target_class)\n",
    "deletion_scores = concept_deletion(U, W, importances, h, imagenet_class=target_class)\n",
    "\n",
    "print(\"Insertion AUC (C_Ins):\", compute_insertion_auc(insertion_scores))\n",
    "print(\"Deletion AUC (C_Del):\", compute_deletion_score(deletion_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a7fedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class ICEEvaluator:\n",
    "    def __init__(self, model, exp, loaders, device='cuda'):\n",
    "        self.model = model\n",
    "        self.exp = exp\n",
    "        self.loaders = loaders\n",
    "        self.device = device\n",
    "\n",
    "        self.h_2d = lambda x: self.model.fc(x)\n",
    "        self.W = torch.tensor(self.exp.reducer._reducer.components_).float().to(self.device)  # [K, C]\n",
    "\n",
    "        self.images, self.A, self.U, self.UW = self._extract_all()\n",
    "\n",
    "    def _extract_all(self):\n",
    "        # Collect all image features\n",
    "        features = []\n",
    "        images = []\n",
    "        for loader in self.loaders:\n",
    "            for x, _ in loader:\n",
    "                images.append(x)\n",
    "                with torch.no_grad():\n",
    "                    model = PytorchModelWrapper(self.model,batch_size=8,predict_target=paras['target_classes'],input_channel_first = False,model_channel_first = True)\n",
    "                    f = model.get_feature(x.to(self.device), self.exp.layer_name)  # [N, H, W, C]\n",
    "                    features.append(f)\n",
    "        A = torch.cat([torch.tensor(f) for f in features], dim=0).to(self.device)  # [N, H, W, C]\n",
    "        images = torch.cat(images, dim=0).to(self.device)\n",
    "\n",
    "        U = torch.tensor(self.exp.reducer.transform(A.detach().cpu().numpy())).float().to(self.device)  # [N, H, W, K]\n",
    "        W = self.W  # [K, C]\n",
    "        UW = torch.einsum('nhwk,kc->nhwc', U, W)  # [N, H, W, C]\n",
    "\n",
    "        return images, A, U, UW\n",
    "\n",
    "    def evaluate_projection(self):\n",
    "        A = self.A\n",
    "        UW = self.UW\n",
    "\n",
    "        A_avg = torch.mean(A, dim=(1, 2))\n",
    "        UW_avg = torch.mean(UW, dim=(1, 2))\n",
    "\n",
    "        logits_A = self.h_2d(A_avg)\n",
    "        logits_UW = self.h_2d(UW_avg)\n",
    "\n",
    "        mse = F.mse_loss(UW_avg, A_avg)\n",
    "        kl = F.kl_div(F.log_softmax(logits_UW, dim=-1), F.softmax(logits_A, dim=-1), reduction='batchmean')\n",
    "        return mse.item(), kl.item()\n",
    "\n",
    "    def compute_sparsity(self):\n",
    "        U_flat = self.U.view(-1, self.U.shape[-1])\n",
    "        non_zero = (U_flat != 0).float().sum(dim=1)\n",
    "        k = self.U.shape[-1]\n",
    "        sparsity_scores = non_zero / k\n",
    "        return sparsity_scores.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c66e6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICEeval = ICEEvaluator( model, e, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf790d",
   "metadata": {},
   "source": [
    "### NMF projection error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e369f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2321694791316986, 0.3566300868988037)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss, kl_loss = ICEeval.evaluate_projection()\n",
    "mse_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f11f9",
   "metadata": {},
   "source": [
    "### Representation sparsity (U_Spr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60ccb8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5597802996635437"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_sparsity = ICEeval.compute_sparsity()\n",
    "rep_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380f30c",
   "metadata": {},
   "source": [
    "### Stability (C-Stab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d531f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_w_list_ice(loaders, model, layer_name, n_components=10, k_folds=5):\n",
    "    \"\"\"\n",
    "    Run ICE multiple times (e.g., k-fold splits) and extract concept bases W from each run.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.decomposition import NMF\n",
    "    import numpy as np\n",
    "\n",
    "    # Step 1: Collect all features into one array\n",
    "    all_features = []\n",
    "    model = PytorchModelWrapper(model,batch_size=8,predict_target=paras['target_classes'],input_channel_first = False,model_channel_first = True)\n",
    "    for loader in loaders:\n",
    "        feats = model.get_feature(loader, layer_name)  # [N, H, W, C]\n",
    "        all_features.append(feats)\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    N, H, W, C = all_features.shape\n",
    "    flat_feats = all_features.reshape(N * H * W, C)\n",
    "\n",
    "    # Step 2: Run KFold splits\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    w_list = []\n",
    "\n",
    "    for i, (train_idx, _) in enumerate(kf.split(flat_feats)):\n",
    "        subset_feats = flat_feats[train_idx]\n",
    "\n",
    "        # Run NMF on this subset\n",
    "        nmf = NMF(n_components=n_components, init='nndsvda', max_iter=500)\n",
    "        nmf.fit(subset_feats)\n",
    "\n",
    "        W = nmf.components_  # Shape: [K, D]\n",
    "        w_list.append(W)\n",
    "\n",
    "    return w_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d1063c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_stability(w_list):\n",
    "    \"\"\"\n",
    "    Computes pairwise cosine similarity between concept bases across k-folds.\n",
    "    Uses Hungarian algorithm to match concept vectors.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    num_models = len(w_list)\n",
    "\n",
    "    for i in range(num_models):\n",
    "        W1 = F.normalize(torch.tensor(w_list[i]), p=2, dim=1).to('cuda')\n",
    "        for j in range(i + 1, num_models):\n",
    "            W2 = F.normalize(torch.tensor(w_list[j]), p=2, dim=1).to('cuda')\n",
    "\n",
    "            sim_matrix = W1 @ W2.T  # shape [K, K]\n",
    "            cost = 1 - sim_matrix.detach().cpu().numpy()\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "            matched_sims = sim_matrix[row_ind, col_ind]\n",
    "            similarities.append(matched_sims.mean().item())\n",
    "\n",
    "    return np.mean(similarities) if similarities else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "415ac57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Score: 0.9802432954311371\n"
     ]
    }
   ],
   "source": [
    "w_list = extract_w_list_ice(loaders, model, paras['layer_name'], k_folds=5)\n",
    "stability_score = compute_stability(w_list)\n",
    "print(\"Stability Score:\", stability_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba618ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11e5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
